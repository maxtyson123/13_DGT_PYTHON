# Based on https://www.youtube.com/watch?v=EeGyoos6r-E&ab_channel=PracticalAIbyRamsri,
# https://colab.research.google.com/drive/1bjGFBtty4Ul156ffir9IKSRKjqv4VkCw
# How to handle the data
# 1. Summarise the paragraph using T5 Transformer
# 2. Paraphrase the summary using T5 Transformer
# 3. Extract Keyword/Phrases using flashtext
# 4. Generate questions using T5 Transformer
# 5. Generate wrong answers using WordNet


# - - - - - - - Imports - - - - - - -#
import os
from textwrap3 import wrap
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
import nltk
import random
import numpy as np
import string
import pke
import traceback
from flashtext import KeywordProcessor
from sense2vec import Sense2Vec
from sentence_transformers import SentenceTransformer
from similarity.normalized_levenshtein import NormalizedLevenshtein
from collections import OrderedDict
from sklearn.metrics.pairwise import cosine_similarity

s2v = Sense2Vec().from_disk('s2v_old')

# Download the required data for the nltk packages
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('brown')
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords

# - - - - - - - Variables - - - - - - -#
test_data_1 = """Elon Musk has shown again he can influence the digital currency market with just his tweets. After 
saying that his electric vehicle-making company Tesla will not accept payments in Bitcoin because of environmental 
concerns, he tweeted that he was working with developers of Dogecoin to improve system transaction efficiency. 
Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, 
while Dogecoin rallied by about 20 percent. The SpaceX CEO has in recent months often tweeted in support of Dogecoin, 
but rarely for Bitcoin.  In a recent tweet, Musk put out a statement from Tesla that it was “concerned” about the 
rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and transaction, and hence was suspending 
vehicle purchases using the cryptocurrency.  A day later he again tweeted saying, “To be clear, I strongly believe in 
crypto, but it can't drive a massive increase in fossil fuel use, especially coal”.  It triggered a downward spiral 
for Bitcoin value but the cryptocurrency has stabilised since.   A number of Twitter users welcomed Musk's statement. 
One of them said it's time people started realising that Dogecoin “is here to stay” and another referred to Musk's 
previous assertion that crypto could become the world's future currency."""

test_data_2 = """I am going to use the python programming language to create an application to train players for a 
quiz. The purpose of this program is to provide a simple user interface for the questions generated by the 
opentdb.com API so that the players can easily practise their quiz skills. To do this the program will use the 
requests module to send a http request to the trivia website and then use a self built renderer in pair with the game 
module to generate a quiz. To give the players the ability to track their progression of skills, stats such as time 
spent, etc. are provided after the game (although can be configured to show after each question). The program also 
provides the ability to add bots to play the game alongside with the player which can be configured to set the 
accuracy of their AI. The program is flexible, allowing for expanding menu sizes and is properly written as to being 
able to handle unexpected user input and corrupted json files. The game has lots of configuration and settings 
allowing for the user to tailor the game to their needs. Players can either take turns on a local machine or 
synchronously play together using the built in TCP-based multiplayer. The program is usable by less technical users 
as a GUI can be used instead of the default python terminal"""

# Set the model and tokenizer
summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')
summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')
question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')
question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')
sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')

# Use the GPU if it is available otherwise use the CPU
processor = "cuda" if torch.cuda.is_available() else "cpu"
device = torch.device(processor)

# Set the model to the device
summary_model = summary_model.to(device)
question_model = question_model.to(device)
normalized_levenshtein = NormalizedLevenshtein()


# - - - - - - - Functions - - - - - -#
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def post_process_text(content):
    final = ""

    # Split the text into sentences
    for sentence in sent_tokenize(content):
        sentence = sentence.capitalize()
        final = final + " " + sentence

    # Return the final text
    return final


def summarizer(text, model, tokenizer):
    # Remove the new lines
    text = text.strip().replace("\n", " ")

    # Add the summarization command
    text = "summarize: " + text

    max_len = 512

    # Tokenize the text
    encoding = tokenizer.encode_plus(text, max_length=max_len, pad_to_max_length=False, truncation=True,
                                     return_tensors="pt").to(device)

    # Get the input ids and attention mask
    input_ids, attention_mask = encoding["input_ids"], encoding["attention_mask"]

    # Generate the summary
    summary_outputs = model.generate(input_ids=input_ids,
                                     attention_mask=attention_mask,
                                     early_stopping=True,
                                     num_beams=3,
                                     num_return_sequences=1,
                                     no_repeat_ngram_size=2,
                                     min_length=75,
                                     max_length=300)

    # Decode the summary
    decoded_summary = [tokenizer.decode(ids, skip_special_tokens=True) for ids in summary_outputs]
    summary = decoded_summary[0]
    summary = post_process_text(summary)
    summary = summary.strip()

    return summary


def extract_nouns(content):
    # Store the result
    out = []

    try:
        # Set up the extractor with the content passed in
        extractor = pke.unsupervised.MultipartiteRank()
        extractor.load_document(input=content, language='en')

        # Allow only nouns and proper nouns
        possible = {'PROPN', 'NOUN'}
        extractor.candidate_selection(pos=possible)

        # Set the weighting method what this does is it will give more weight to the nouns that are more frequent
        extractor.candidate_weighting(alpha=1.1,
                                      threshold=0.75,
                                      method='average')
        # Get the top 15 nouns
        key_phrases = extractor.get_n_best(n=15)

        # Append the nouns to the output (index 0 is the noun)
        for val in key_phrases:
            out.append(val[0])

    except:
        # If there is an error then return an empty list
        out = []
        traceback.print_exc()

    return out


def get_keywords(original_text, summary_text):
    # Get all the nouns from the original text
    original_text_nouns = extract_nouns(original_text)

    # Set up the processor and add the nouns
    keyword_processor = KeywordProcessor()
    for noun in original_text_nouns:
        keyword_processor.add_keyword(noun)

    # Get the keywords from the summary text
    keywords_found = keyword_processor.extract_keywords(summary_text)
    keywords_found = list(set(keywords_found))

    important_keywords = []

    # Compare the keywords found in the summary to the nouns in the original text. If they match then add them to the
    # important keywords
    for keyword in original_text_nouns:
        if keyword in keywords_found:
            important_keywords.append(keyword)

    return important_keywords


def get_question(context, answer, model, tokenizer):
    # Create the prompt for the model
    text = "context: {} answer: {}".format(context, answer)

    # Tokenize the text, ensure no truncation and no padding
    encoding = tokenizer.encode_plus(text, max_length=384, pad_to_max_length=False, truncation=True,
                                     return_tensors="pt").to(device)

    # Get the input ids and attention mask
    input_ids, attention_mask = encoding["input_ids"], encoding["attention_mask"]

    # Generate the question
    output_questions = model.generate(input_ids=input_ids,
                                      attention_mask=attention_mask,
                                      early_stopping=True,
                                      num_beams=5,
                                      num_return_sequences=1,
                                      no_repeat_ngram_size=2,
                                      max_length=72)

    # Decode the question
    decoded_questions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_questions]

    # Remove the "question:" prompt from the question, strip the white space and return the question
    question = decoded_questions[0].replace("question:", "")
    question = question.strip()
    return question


def filter_same_sense_words(original, wordlist):
    filtered_words = []
    # Get the base sense of the original word
    base_sense = original.split('|')[1]

    # Loop through each word in the wordlist and check if the sense matches the base sense
    for each_word in wordlist:
        if each_word[0].split('|')[1] == base_sense:
            # If the sense matches then add the word to the filtered words list. Also remove the sense and replace
            # the underscores with spaces
            filtered_words.append(each_word[0].split('|')[0].replace("_", " ").title().strip())
    return filtered_words


def get_highest_similarity_score(wordlist, wrd):
    score = []

    # Loop through each word in the wordlist and get the similarity score to the original word
    for each in wordlist:
        score.append(normalized_levenshtein.similarity(each.lower(), wrd.lower()))

    # Return the highest similarity score
    return max(score)


def sense2vec_get_words(word, s2v, topn, question):
    try:
        # Get the best sense of the given word based on the provided senses
        sense = s2v.get_best_sense(word,
                                   senses=["NOUN", "PERSON", "PRODUCT", "LOC", "ORG", "EVENT", "NORP", "WORK OF ART",
                                           "FAC", "GPE", "NUM", "FACILITY"])

        # Get the most similar words to the given word based on the best sense
        most_similar = s2v.most_similar(sense, n=topn)

        # Filter out words that have the same sense as the given word
        output = filter_same_sense_words(sense, most_similar)
    except:
        output = []

    # Set a threshold for the similarity score between words
    threshold = 0.6

    # Initialize a list with the given word
    final = [word]

    # Split the question into a list of words
    checklist = question.split()

    # Loop through the list of similar words and add them to the final list if they meet the similarity score threshold
    # and are not already in the final list or the question
    for x in output:
        if get_highest_similarity_score(final, x) < threshold and x not in final and x not in checklist:
            final.append(x)

    # Return the list of similar words (excluding the original word)
    return final[1:]


def mmr(doc_embedding, word_embeddings, words, num_of_keywords, lambda_param):
    # Calculate the similarity between the words in the document and the document itself
    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)

    # Calculate the similarity between all pairs of words in the document
    word_similarity = cosine_similarity(word_embeddings)

    # Initialize the first keyword/key-phrase by selecting the word with the highest similarity to the document
    keywords_idx = [np.argmax(word_doc_similarity)]

    # Create a list of candidate words that have not been selected as a keyword yet
    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]

    # Loop through the top_n-1 iterations to select the top n keywords/key phrases
    for item in range(num_of_keywords - 1):
        # Extract the similarities between the candidate words and the document, as well as between the
        # candidate words and the selected keywords/keyphrases
        candidate_similarities = word_doc_similarity[candidates_idx, :]
        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)

        # Calculate the Maximum Marginal Relevance (MMR) score for each candidate word
        mmr = lambda_param * candidate_similarities - (1 - lambda_param) * target_similarities.reshape(-1, 1)

        # Select the candidate word with the highest MMR score
        mmr_idx = candidates_idx[np.argmax(mmr)]

        # Add the selected keyword/keyphrase to the list of keywords and remove it from the list of candidates
        keywords_idx.append(mmr_idx)
        candidates_idx.remove(mmr_idx)

    # Return the list of selected keywords/keyphrases
    return [words[idx] for idx in keywords_idx]


def get_distractors(word, original_sentence, sense2vec_model, sentence_model, number_of_words, weight):
    # Get the distractors using sense to vector
    distractors = sense2vec_get_words(word, sense2vec_model, number_of_words, original_sentence)

    # If there are no distractors then return an empty list
    if len(distractors) == 0:
        return distractors

    # Add the original word to the distractors and then add all the distractors
    distractors_new = [word.capitalize()]
    distractors_new.extend(distractors)

    # Add the original word to the sentence and then encode the sentence
    embedding_sentence = original_sentence + " " + word.capitalize()
    keyword_embedding = sentence_model.encode([embedding_sentence])

    # Encode the distractors
    distractor_embeddings = sentence_model.encode(distractors_new)

    # Get the filtered keywords using the MMR algorithm. The disired amount of keywords is either 3 or the number of
    # distractors if it is less than 3
    max_keywords = min(len(distractors_new), 4)
    filtered_keywords = mmr(keyword_embedding, distractor_embeddings, distractors_new, max_keywords, weight)

    # Create a new array and add the original word to the start of the array
    final = [word.capitalize()]

    # Loop through the filtered keywords and add them to the final array only if they are not the original word
    for filtered_word in filtered_keywords:
        if filtered_word.lower() != word.lower():
            final.append(filtered_word.capitalize())

    # Remove the original word from the array
    final = final[1:]
    return final


def main():
    os.system("cls")

    set_seed(42)
    data = test_data_1
    print(f"Using {processor} to process the data")

    print("INPUT DATA: ")
    for wrapped_text in wrap(data, 150):
        print(wrapped_text)
    print("\n")

    print("Summarising the data...")
    summary_of_data = summarizer(data, summary_model, summary_tokenizer)

    print("SUMMARY: ")
    for wrapped_text in wrap(summary_of_data, 150):
        print(wrapped_text)
    print("\n")

    print("Getting the important keywords...")
    important_keywords = get_keywords(data, summary_of_data)
    print(important_keywords)

    print("Getting the questions...")
    for answer in important_keywords:
        question = get_question(summary_of_data, answer, question_model, question_tokenizer)
        distractions = get_distractors(answer.capitalize(), question, s2v, sentence_transformer_model, 40, 0.2)
        if len(distractions) != 3:
            print("Not enough distractors")
            continue

        other_choices = ", ".join(distractions)
        print(f"Question: {question} Answer: {answer.capitalize()} | Other Choices: {other_choices}")


if __name__ == "__main__":
    main()
# Technically I can call this a GPT because it's a transformer model, but it's not the same as ChatGPT
